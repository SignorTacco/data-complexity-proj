{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to download all the transcription for all the episodes and seeing which ones have a link to a character. If not, then we have to review it one by one and see which character they mean and add the edge manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11 transcript titles from pickle file.\n",
      "Fetching page for: Anatomy_Park_(episode)/Transcript\n",
      "Saved: rick_and_morty_transcripts_season_11\\Anatomy_Park_(episode)Transcript.txt\n",
      "Fetching page for: Close_Rick-Counters_of_the_Rick_Kind/Transcript\n",
      "Saved: rick_and_morty_transcripts_season_11\\Close_Rick-Counters_of_the_Rick_KindTranscript.txt\n",
      "Fetching page for: Lawnmower_Dog/Transcript\n",
      "Saved: rick_and_morty_transcripts_season_11\\Lawnmower_DogTranscript.txt\n",
      "Fetching page for: M._Night_Shaym-Aliens!/Transcript\n",
      "Saved: rick_and_morty_transcripts_season_11\\M._Night_Shaym-Aliens!Transcript.txt\n",
      "Fetching page for: Meeseeks_and_Destroy/Transcript\n",
      "Saved: rick_and_morty_transcripts_season_11\\Meeseeks_and_DestroyTranscript.txt\n",
      "Fetching page for: Pilot/Transcript\n",
      "Saved: rick_and_morty_transcripts_season_11\\PilotTranscript.txt\n",
      "Fetching page for: Raising_Gazorpazorp/Transcript\n",
      "Saved: rick_and_morty_transcripts_season_11\\Raising_GazorpazorpTranscript.txt\n",
      "Fetching page for: Rick_Potion_No._9/Transcript\n",
      "Saved: rick_and_morty_transcripts_season_11\\Rick_Potion_No._9Transcript.txt\n",
      "Fetching page for: Ricksy_Business/Transcript\n",
      "Saved: rick_and_morty_transcripts_season_11\\Ricksy_BusinessTranscript.txt\n",
      "Fetching page for: Rixty_Minutes/Transcript\n",
      "Saved: rick_and_morty_transcripts_season_11\\Rixty_MinutesTranscript.txt\n",
      "Fetching page for: Something_Ricked_This_Way_Comes/Transcript\n",
      "Saved: rick_and_morty_transcripts_season_11\\Something_Ricked_This_Way_ComesTranscript.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pickle\n",
    "\n",
    "# Base URL for the Rick and Morty Wiki\n",
    "base_url = \"https://rickandmorty.fandom.com\"\n",
    "category_url = f\"{base_url}/wiki/Category:Season_1_transcripts\"\n",
    "\n",
    "# Directory to save transcript files\n",
    "save_dir = \"rick_and_morty_transcripts_season_1\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Fandom API endpoint\n",
    "fandom_api_url = \"https://rickandmorty.fandom.com/api.php\"\n",
    "\n",
    "# File to save collected episode titles\n",
    "pickle_file = \"all_transcript_titles.pkl\"\n",
    "\n",
    "# Function to scrape transcript titles and URLs from the category page\n",
    "def get_transcript_titles(page_url):\n",
    "    transcript_titles = []\n",
    "    response = requests.get(page_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Loop through each transcript link\n",
    "    for link in soup.select('a[href*=\"/Transcript\"]'):\n",
    "        href = link.get('href')\n",
    "        name = link.text.strip()\n",
    "\n",
    "        if href:\n",
    "            transcript_titles.append({\n",
    "                \"name\": name,\n",
    "                \"url\": urljoin(base_url, href)\n",
    "            })\n",
    "    return transcript_titles\n",
    "\n",
    "# Function to fetch transcript page content using the Fandom API\n",
    "def fetch_transcript_content(title):\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"revisions\",\n",
    "        \"titles\": title,\n",
    "        \"rvslots\": \"main\",\n",
    "        \"rvprop\": \"content\",\n",
    "        \"format\": \"json\",\n",
    "        \"formatversion\": \"2\"\n",
    "    }\n",
    "    response = requests.get(fandom_api_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        pages = data.get(\"query\", {}).get(\"pages\", [])\n",
    "        if pages and \"revisions\" in pages[0]:\n",
    "            return pages[0][\"revisions\"][0][\"slots\"][\"main\"][\"content\"]\n",
    "    return None\n",
    "\n",
    "# Function to format names for Fandom URL\n",
    "def format_name(name):\n",
    "    return name.replace(\" \", \"_\")\n",
    "\n",
    "# Function to sanitize filenames\n",
    "def sanitize_filename(filename):\n",
    "    return re.sub(r'[<>:\"/\\\\|?*]', '', filename)\n",
    "\n",
    "# Function to save content as a plain-text file\n",
    "def save_content(content, filename):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "\n",
    "# Step 1: Check if pickle file exists; if so, load data\n",
    "if os.path.exists(pickle_file):\n",
    "    with open(pickle_file, \"rb\") as f:\n",
    "        all_transcript_titles = pickle.load(f)\n",
    "    print(f\"Loaded {len(all_transcript_titles)} transcript titles from pickle file.\")\n",
    "else:\n",
    "    # Step 2: Collect transcript titles from all pages\n",
    "    all_transcript_titles = []\n",
    "    next_url = category_url\n",
    "\n",
    "    while next_url:\n",
    "        transcript_titles, next_url = get_transcript_titles(next_url)\n",
    "        all_transcript_titles.extend(transcript_titles)\n",
    "        print(f\"Collected {len(transcript_titles)} transcripts from the current page.\")\n",
    "        print(\"Next URL:\", next_url)  # Debug print to check the next URL\n",
    "\n",
    "    # Remove duplicates and save to pickle file\n",
    "    all_transcript_titles = [dict(t) for t in {tuple(d.items()) for d in all_transcript_titles}]\n",
    "    print(f\"Total unique transcripts found: {len(all_transcript_titles)}\")\n",
    "\n",
    "    with open(pickle_file, \"wb\") as f:\n",
    "        pickle.dump(all_transcript_titles, f)\n",
    "    print(f\"Transcript titles saved to {pickle_file}\")\n",
    "\n",
    "# Step 3: Fetch and save content for each transcript\n",
    "for transcript in all_transcript_titles:\n",
    "    formatted_name = format_name(transcript[\"name\"])\n",
    "    sanitized_name = sanitize_filename(formatted_name)\n",
    "    print(f\"Fetching page for: {formatted_name}\")\n",
    "    content = fetch_transcript_content(formatted_name)\n",
    "\n",
    "    if content:\n",
    "        # Save to file with sanitized transcript name\n",
    "        filename = os.path.join(save_dir, f\"{sanitized_name}.txt\")\n",
    "        save_content(content, filename)\n",
    "        print(f\"Saved: {filename}\")\n",
    "    else:\n",
    "        print(f\"Transcript not found or failed for: {formatted_name}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
